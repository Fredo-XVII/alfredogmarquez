model <- spark.glm(df, eruptions ~ waiting)
summary(model)
blogdown::build_site()
sparkR.uiWebUrl()
blogdown::build_site()
sparkR.uiWebUrl()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
sparkR.uiWebUrl()
head(collect(waiting_70), 10)
blogdown::build_site()
sparkR.session.stop()
sparkR.session(enableHiveSupport = FALSE ,
master = "local[*]",
sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
sparkConfig =
list(spark.driver.memory = "2g", spark.executor.memory = "2g",
spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
)
sparkR.uiWebUrl()
sparkR.uiWebUrl()
sparkR.uiWebUrl()
df <- as.DataFrame(faithful)
str(df)
colnames(df)
sparkR.uiWebUrl()
sparkR.session.stop()
sparkR.uiWebUrl()
sparkR.session(appName = "Blog",
enableHiveSupport = FALSE ,
master = "local[*]",
sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
sparkConfig =
list(spark.driver.memory = "2g", spark.executor.memory = "2g",
spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
)
sparkR.uiWebUrl()
SparkR::sparkRHive.init()
?sparkR.uiWebUrl
sparkR.uiWebUrl("Blog")
sparkR.spark.session
?sparkR.session
sparkR.session.stop()
sparkR.session(appName = "Blog",
enableHiveSupport = FALSE ,
master = "local[*]",
sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
sparkConfig =
list(spark.driver.memory = "2g" #, spark.executor.memory = "2g",
spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
sparkR.session(appName = "Blog",
enableHiveSupport = FALSE ,
master = "local[*]",
sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
sparkConfig =
list(spark.driver.memory = "2g", # spark.executor.memory = "2g",
spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
)
sparkR.uiWebUrl()
sparkR.session.stop()
sparkR.session(#appName = "Blog",
enableHiveSupport = FALSE ,
master = "local[*]",
sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
sparkConfig =
list(spark.driver.memory = "2g", # spark.executor.memory = "2g",
spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
)
sparkR.uiWebUrl()
blogdown::build_site()
waiting_70 <- sql("select * from df where waiting > 70")
df <- as.DataFrame(faithful)
str(df)
colnames(df)
createOrReplaceTempView(df, "df")
waiting_70 <- sql("select * from df where waiting > 70")
str(waiting_70)
df %>% collect() %>%
ggplot(aes(x = waiting, y = eruptions)) +
geom_point()
library(tidyverse)
df %>% collect() %>%
ggplot(aes(x = waiting, y = eruptions)) +
geom_point()
library(tidyverse)
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
SparkR::sparkR.session.stop()
library(tidyverse)
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.session(#appName = "Blog",
enableHiveSupport = FALSE ,
master = "local[*]",
sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
sparkConfig =
list(spark.driver.memory = "2g", # spark.executor.memory = "2g",
spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
)
sparkR.uiWebUrl()
df <- as.DataFrame(faithful)
str(df)
colnames(df)
createOrReplaceTempView(df, "df")
waiting_70 <- sql("select * from df where waiting > 70")
str(waiting_70)
head(SparkR::collect(waiting_70), 10)
collect(summary(df))
SparkR::collect(SparkR::summary(df))
SparkR::corr(df, "waiting", "eruptions")
df %>% SparkR::collect() %>%
ggplot(aes(x = waiting, y = eruptions)) +
geom_point()
df %>% SparkR::collect() %>%
ggplot(aes(x = waiting, y = eruptions)) +
geom_point() +
geom_smooth()
head(df)
blogdown::build_site()
sparkR.uiWebUrl()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
waiting_avg <- SparkR::avg(df$waiting)
eruptions_avg <- SparkR::avg(df$eruptions)
df %>% SparkR::collect() %>%
ggplot(aes(x = waiting, y = eruptions)) +
geom_point() +
geom_smooth() +
geom_vline(xintercept = waiting_avg) +
geom_hline(yintercept = eruptions)
search()
waiting_avg <- SparkR::avg(df$waiting)
eruptions_avg <- SparkR::avg(df$eruptions)
df %>% SparkR::collect() %>%
ggplot(aes(x = waiting, y = eruptions)) +
geom_point() +
geom_smooth() +
geom_vline(xintercept = waiting_avg) +
geom_hline(yintercept = eruptions_avg)
waiting_avg <- SparkR::avg(df$waiting) %>% SparkR::collect()
waiting_avg
head(waiting_avg)
SparkR::avg(df$waiting)
head(SparkR::avg(df$waiting))
str(sf)
str(df)
avg(df$waiting)
collect(avg(df$waiting) )
SparkR::collect(avg(df$waiting) )
mean(df$eruptions)
head(df)
SparkR::avg(df,"waiting")
SparkR::collect(df$waiting) %>% mean()
collect(df$waiting)
waiting_avg <- SparkR::mean(df$waiting)
waiting_avg
waiting_avg <- SparkR::mean(df$waiting) %>% SparkR::collect()
SparkR::select(mean(df$waiting))
SparkR::select(SparkR::mean(df$waiting)) %>% SparkR::collect()
waiting_avg <- SparkR::select(df, SparkR::mean(df$waiting)) %>% SparkR::collect()
waiting_avg
waiting_avg <- SparkR::select(df, SparkR::mean(df$waiting)) %>% SparkR::collect()
eruptions_avg <- SparkR::select(df, SparkR::mean(df$eruptions)) %>% SparkR::collect()
df %>% SparkR::collect() %>%
ggplot(aes(x = waiting, y = eruptions)) +
geom_point() +
geom_smooth() +
geom_vline(xintercept = waiting_avg) +
geom_hline(yintercept = eruptions_avg)
waiting_avg
str(waiting_avg)
waiting_avg <- SparkR::select(df, SparkR::mean(df$waiting)) %>% SparkR::collect()
eruptions_avg <- SparkR::select(df, SparkR::mean(df$eruptions)) %>% SparkR::collect()
df %>% SparkR::collect() %>%
ggplot(aes(x = waiting, y = eruptions)) +
geom_point() +
geom_smooth() +
geom_vline(xintercept = waiting_avg$avg(waiting)) +
geom_hline(yintercept = eruptions_avg$`avg(eruptions)`)
waiting_avg <- SparkR::select(df, SparkR::mean(df$waiting)) %>% SparkR::collect()
eruptions_avg <- SparkR::select(df, SparkR::mean(df$eruptions)) %>% SparkR::collect()
df %>% SparkR::collect() %>%
ggplot(aes(x = waiting, y = eruptions)) +
geom_point() +
geom_smooth() +
geom_vline(xintercept = waiting_avg$`avg(waiting)`) +
geom_hline(yintercept = eruptions_avg$`avg(eruptions)`)
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
SparR::spark.session.stop
SparR::spark.session.stop()
SparkR::spark.session.stop()
SparkR::sparkR.session.stop()
rm(list=ls())
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown:::new_post_addin()
blogdown::serve_site()
blogdown::serve_site()
blogdown::hugo_build()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown::hugo_build()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
install.packages("stringi")
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
install.packages(c("assertthat", "cli", "colorspace", "e1071", "fs", "git2r", "gtable", "highr", "httpuv", "JuliaCall", "lazyeval", "Matrix", "mgcv", "openssl", "pkgbuild", "purrr", "rgdal", "rlang", "rstudioapi", "tibble"))
blogdown::build_site()
install.packages("tidyverse")
install.packages("backports")
blogdown::build_site()
blogdown::build_site()
install.packages("pander")
# installing/loading the package:
if(!require(installr)) { install.packages("installr"); require(installr)} #load / install+load installr
# Installing pandoc
install.pandoc()
blogdown::build_site()
knitr::include_graphics("\images\Apache_folder_2018_05_15.png")
knitr::include_graphics("/images/Apache_folder_2018_05_15.png")
knitr::include_graphics("/public/images/Apache_folder_2018_05_15.png")
library("png", lib.loc="C:/Apps/R/R-3.5.0/library")
detach("package:png", unload=TRUE)
blogdown::build_site()
sparkR.session(
enableHiveSupport = FALSE ,
master = "local[*]",
sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
sparkConfig =
list(spark.driver.memory = "2g",
spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
)
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.session(
enableHiveSupport = FALSE ,
master = "local[*]",
sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
sparkConfig =
list(spark.driver.memory = "2g",
spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
)
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown:::insert_image_addin()
getwd()
library(blogdown)
getwd()
print("hello world")
pandoc_version()
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
blogdown::build_site()
getwd()
unlink('content/post/2019-09-29-juliapro-on-jupyter_cache', recursive = TRUE)
install.packages("blogdown")
install.packages("blogdown")
install.packages("blogdown")
library(blogdown)
getwd()
getwd()
library(blogdown)
hugo_build()
hugo_build()
build_site()
library(blogdown)
build_site()
getwd()
install.packages("cranlogs")
library(cranlogs)
cran_R_downloads <- cranlogs::cran_downloads('R')
View(cran_R_downloads)
73 * 365
73 * 365 * 5
lubridate::today()
lubridate::today() - 1
cran_R_downloads <- cranlogs::cran_downloads('R', from = "2015-01-01", to = lubridate::today())
library(tidyverse)
# EDA
cran_R_downloads %>% head() %>% View()
cran_R_downloads %>% select(distinct(os))
cran_R_downloads %>% select(distinct(.$os))
cran_R_downloads %>% select(distinct('os'))
# Download R download data
cran_R_downloads_raw <- cranlogs::cran_downloads('R', from = "2015-01-01", to = lubridate::today())
cran_R_downloads <- cran_R_downloads_raw %>% mutate(os_factor = forcats::fct_inorder(forcats::as_factor('os')))
# EDA
cran_R_downloads %>% head() %>% View()
cran_R_downloads %>% forcats::fct_count('os')
cran_R_downloads %>% forcats::fct_count(os)
cran_R_downloads %>% forcats::fct_count(.$os)
cran_R_downloads %>% select(os) %>% forcats::fct_count()
cran_R_downloads %>% select(os) %>% table()
cran_R_downloads %>% select(date) %>% distinct()
cran_R_downloads %>% select(date) %>% distinct() %>% length()
cran_R_downloads %>% select(date) %>% distinct() %>% nrow()
cran_R_downloads %>% select(version) %>% dplyr::count()
cran_R_downloads %>% select(os) %>% group_by(os) %>% summarise(os_cnts = n())
cran_R_downloads %>% select(version) %>% dplyr::group_by(version) %>% summarise(version_cnts = n())
cran_R_downloads <- cran_R_downloads_raw %>% mutate(os_factor = forcats::fct_inorder(forcats::as_factor('os')),
ver_factor = forcats::fct_inorder(forcats::as_factor('version'))
)
cran_R_downloads <- cran_R_downloads_raw %>% mutate(os_factor = forcats::as_factor('os'),
ver_factor = forcats::fct_inorder(forcats::as_factor('version'))
)
cran_R_downloads <- cran_R_downloads_raw %>% mutate(os_factor = forcats::as_factor('os'),
ver_factor = forcats::as_factor('version')
)
# EDA
cran_R_downloads %>% head() %>% View()
cran_R_downloads <- cran_R_downloads_raw %>% mutate(os_factor = forcats::as_factor(os),
ver_factor = forcats::as_factor('version')
)
cran_R_downloads <- cran_R_downloads_raw %>% mutate(os_factor = forcats::as_factor(os),
ver_factor = forcats::as_factor(version)
)
cran_R_downloads <- cran_R_downloads_raw %>% mutate(os_factor = forcats::fct_inorder(forcats::as_factor(os)),
ver_factor = forcats::as_factor(version)
)
cran_R_downloads <- cran_R_downloads_raw %>% mutate(os_factor = forcats::fct_inorder(forcats::as_factor(os)),
ver_factor =forcats::fct_inorder(forcats::as_factor(version))
)
# EDA
cran_R_downloads %>% head() %>% View()
cran_R_downloads %>% select(os) %>% group_by(os) %>% summarise(os_cnts = n())
cran_R_downloads %>% select(date) %>% distinct() %>% nrow()
cran_R_downloads %>% select(version) %>% dplyr::group_by(version) %>% summarise(version_cnts = n())
# Build Aggregation Views
down_tot_by_d <- cran_R_downloads %>%
dplyr::group_by(date) %>%
dplyr::summarise(counts = n()) %>% head()
# Build Aggregation Views
down_tot_by_d <- cran_R_downloads %>%
dplyr::group_by(date) %>%
dplyr::summarise(counts = n())
head(down_tot_by_d)
down_tot_by_d %>% ggplot(aes(x = date, y = counts)) +
geom_line()
# What does the trend look like by os
down_tot_by_os <- cran_R_downloads %>%
dplyr:: group_by(os) %>%
dplyr::summarise(counts = n())
# What does the trend look like by os
down_tot_by_os <- cran_R_downloads %>%
dplyr:: group_by(date,os) %>%
dplyr::summarise(counts = n())
down_tot_by_os %>% ggplot(aes(x = date, y = counts, groups = os)) +
geom_line()
down_tot_by_os %>% ggplot(aes(x = date, y = counts, group = os)) +
geom_line()
# What does the trend look like by os
down_tot_by_os <- cran_R_downloads %>%
dplyr:: group_by(date,os_factor) %>%
dplyr::summarise(counts = n())
down_tot_by_os %>% ggplot(aes(x = date, y = counts, col = os_factor, group = os_factor)) +
geom_line()
down_tot_by_os %>% ggplot(aes(x = date, y = counts, col = os_factor, group = os_factor)) +
geom_line() +
scale_x_log10()
down_tot_by_os %>% ggplot(aes(x = date, y = counts, col = os_factor, group = os_factor)) +
geom_line() +
scale_y_log10()
down_tot_by_os %>% ggplot(aes(x = date, y = counts, col = os_factor, group = os_factor)) +
geom_line() +
facet_wrap() +
scale_y_log10()
down_tot_by_os %>% ggplot(aes(x = date, y = counts, col = os_factor, group = os_factor)) +
geom_line() +
facet_wrap(.~os_factor) +
scale_y_log10()
down_tot_by_os %>% ggplot(aes(x = date, y = counts, col = os_factor, group = os_factor)) +
geom_line() +
facet_wrap(.~os_factor) #+
# Build Aggregation Views
# What does the trend look like?
down_tot_by_d <- cran_R_downloads %>%
dplyr::group_by(date) %>%
dplyr::summarise(counts = sum(count))
down_tot_by_d %>% ggplot(aes(x = date, y = counts)) +
geom_line()
# What does the trend look like by os
down_tot_by_os <- cran_R_downloads %>%
dplyr:: group_by(date,os_factor) %>%
dplyr::summarise(counts = sum(count))
down_tot_by_os %>% ggplot(aes(x = date, y = counts, col = os_factor, group = os_factor)) +
geom_line() +
facet_wrap(.~os_factor) #+
down_tot_by_os %>% ggplot(aes(x = date, y = counts, col = os_factor, group = os_factor)) +
geom_line() +
facet_wrap(.~os_factor) +
scale_y_log10()
down_tot_by_d %>% ggplot(aes(x = date, y = counts)) +
geom_line() +
scale_y_log10()
# Build Aggregation Views
# What does the trend look like?
down_tot_by_d <- cran_R_downloads %>%
dplyr::group_by(date) %>%
dplyr::summarise(total = sum(count))
down_tot_by_d %>% ggplot(aes(x = date, y = total)) +
geom_line() +
scale_y_log10()
down_tot_by_d %>% ggplot(aes(x = date, y = total)) +
geom_line()
down_tot_by_ver <- cran_R_downloads %>%
dplyr:: group_by(date,ver_factor) %>%
dplyr::summarise(counts = sum(count))
down_tot_by_ver %>% ggplot(aes(x = date, y = counts, col = ver_factor, group = os_factor)) +
geom_line() +
facet_wrap(.~os_factor) +
scale_y_log10()
down_tot_by_ver %>% ggplot(aes(x = date, y = counts, col = ver_factor, group = ver_factor)) +
geom_line() +
facet_wrap(.~os_factor) +
scale_y_log10()
down_tot_by_ver %>% ggplot(aes(x = date, y = counts, col = ver_factor, group = ver_factor)) +
geom_line() +
facet_wrap(.~ver_factor) +
scale_y_log10()
View(down_tot_by_ver)
View(cran_R_downloads)
# What does the trend look like by os
down_tot_by_os <- cran_R_downloads %>%
dplyr:: group_by(date,os_factor) %>%
dplyr::summarise(total = sum(count))
down_tot_by_os %>% ggplot(aes(x = date, y = total, col = os_factor, group = os_factor)) +
geom_line() +
facet_wrap(.~os_factor) +
scale_y_log10()
# What does the trend look like by version
down_tot_by_ver <- cran_R_downloads %>%
dplyr:: group_by(date,ver_factor) %>%
dplyr::summarise(total = sum(count))
down_tot_by_ver %>% ggplot(aes(x = date, y = total, col = ver_factor, group = ver_factor)) +
geom_line() +
facet_wrap(.~ver_factor) +
scale_y_log10()
cran_R_downloads <- cran_R_downloads_raw %>%
mutate(os_factor = forcats::fct_inorder(forcats::as_factor(os)),
ver_factor =forcats::fct_inorder(forcats::as_factor(version)),
year = lubridate::year(date),
month = lubridate::month(date),
week = lubridate::week(date),
yr_mo = stringr::str_c(year,'_',month),
yr_wk = stringr::str_c(year,'_',weel),
)
cran_R_downloads <- cran_R_downloads_raw %>%
mutate(os_factor = forcats::fct_inorder(forcats::as_factor(os)),
ver_factor =forcats::fct_inorder(forcats::as_factor(version)),
year = lubridate::year(date),
month = lubridate::month(date),
week = lubridate::week(date),
yr_mo = stringr::str_c(year,'_',month),
yr_wk = stringr::str_c(year,'_',week),
)
View(cran_R_downloads)
# Build Aggregation Views
# What does the trend look like?
down_tot_by_d <- cran_R_downloads %>%
dplyr::group_by(yr_wk) %>%
dplyr::summarise(total = sum(count))
down_tot_by_d %>% ggplot(aes(x = yr_wk, y = total)) +
geom_line() +
scale_y_log10()
install.packages("tsibble")
install.packages("tsibble")
install.packages("tsibble")
# Build Aggregation Views
# What does the trend look like?
down_tot_by_d <- cran_R_downloads %>%
dplyr::group_by(yr_wk) %>%
dplyr::summarise(total = sum(count)) %>%
tsibble::as_tsibble()
# Count R download Stats
library(cranlogs)
library(lubridate)
library(tidyverse)
library(tsibble)
down_tot_by_d <- cran_R_downloads %>%
dplyr::group_by(yr_wk) %>%
dplyr::summarise(total = sum(count))
plot(down_tot_by_d)
plot(down_tot_by_d$total)
down_tot_by_d %>% ggplot(aes(x = yr_wk, y = total)) +
geom_line()
library(cranlogs)
library(lubridate)
library(tidyverse)
