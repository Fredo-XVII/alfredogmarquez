---
title: "Spark Installation on Windows"
author: "Alfredo G Marquez"
date: '2018-05-04'
slug: spark-installation-on-windows
tags: 
  - Spark
  - SparkR
  - installation
  - windows
categories:
- Spark
- SparkR
keywords:
- SparkR
- Spark
- Windows
- Installation
autoThumbnailImage: false
thumbnailImagePosition: "top"
thumbnailImage: https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg
coverImage: https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg
metaAlignment: center
coverMeta: out
summary: "Today I will discuss how to install Apache Spark onto a Windows machine."
draft: false
---



<div id="intro" class="section level1">
<h1>Intro</h1>
<blockquote>
<p>Today I will discuss how to install Apache Spark onto a Windows machine. I have just walked through the process a second time at work due to a laptop swap and it takes me sometime to remember all the steps to get the install right, so I thought I would document the process.</p>
</blockquote>
<pre class="md"><code>&lt;!--more--&gt;</code></pre>
<hr />
</div>
<div id="step-1-download-and-installation" class="section level1">
<h1>Step #1: Download and Installation</h1>
<p><strong>Install Spark</strong></p>
<p>First you will need to download Spark, which comes with the package for SparkR. Note, as of this posting, the SparkR package was removed from CRAN, so you can only get SparkR from the Apache website. Spark can be downloaded directly from Apache <a href="http://spark.apache.org/downloads.html">here</a>.</p>
<p>After downloading, save the zipped file to a directory of choice, and then unzip the file. I have an Apps (“C:\Apps”) folder where I put all the software that I download and install. This make it easy to transition to another laptop as all I have to do is go down the list installing software. I changed/shortened the Spark folder name for simplicity and ease of typing.</p>
<p><img src="/images/Apache_folder_2018_05_15.png" /><!-- --> <br></br></p>
<p><strong>Install Java</strong></p>
<p>As of this writing, Java 9 has been having a ton of issues and is not stable. I was able to use the most recent version of Java 8. You can download the most recent version for Windows at this <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">link</a>. You have to accept their license agreement before downloadning.</p>
<p>Unlike Spark, I will let the system determine where to install Java. I feel like this works best to reduce future issues.</p>
<p><strong>Install WINUTILS</strong></p>
<p>This step tripped me up for a long time when I first started to learning about Spark. Not many blog posts would talk about installing this software. I am not really sure what it does, but if you want more information here is the <a href="https://wiki.apache.org/hadoop/WindowsProblems">ApacheWiki</a>. You can download the winutils software from the github repo <a href="https://github.com/steveloughran/winutils">here</a>. Click on the green button (“clone or download”) on the upper right of the page and download the zip file. I would put this in your “C:Apps\winutils” folder so you can reference it easily (you will need it later). Extract all the files to this location using decompression software like <a href="http://www.peazip.org/">PeaZip</a> or WinZip. Pea7 is free and open source.</p>
<p>Once you have unzipped the file it should look like this: <img src="/images/winutils_folder_2018_05_15.png" /><!-- --> <br></br></p>
</div>
<div id="step-2-set-up-system-environmental-variables-to-run-spark-either-in-windows-or-r" class="section level1">
<h1>Step #2: Set up system environmental variables to run Spark (either in windows, or R):</h1>
<blockquote>
<p>NOTE: if you decide to set the environmental variables through R, you need to remember that it does not change them on your machine, nor does it flow through to the next session unless they are establish in your .Renviron script. The code to do it in R is below, however, I will discuss setting up on your machine next.</p>
</blockquote>
<blockquote>
<p>Sys.setenv(SPARK_HOME = “C:\Apps\Apache\Spark\spark-2.3.0\bin”)</p>
</blockquote>
<div id="spark_home" class="section level4">
<h4>SPARK_HOME:</h4>
<p>The process to set up the environmental variables on your machine will be the same for Spark as it is for the other variables below. So I will use Spark as a example that you can apply to the other variables.</p>
<ol style="list-style-type: decimal">
<li>Locate the “bin” file path for the software you want to add to your environmental list. For Spark, the path is in my “C:\Apps” folder, or “C:\Apps\Apache\Spark\spark-2.3.0\bin”</li>
</ol>
<p>## SPARKR_DRIVER_R: R_HOME from sys.getenv</p>
<p>## JAVA_HOME:</p>
<p>## WINUTILS_HOME:</p>
</div>
</div>
<div id="testing-install" class="section level1">
<h1>Testing Install</h1>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<ul>
<li>RPubs : <a href="https://rpubs.com/wendyu/sparkr" class="uri">https://rpubs.com/wendyu/sparkr</a></li>
</ul>
</div>
<div id="set-library-path-and-load-sparkr-library" class="section level1">
<h1>Set library path and load SparkR library</h1>
<p>.libPaths(c(file.path(Sys.getenv(“SPARK_HOME”), “R”, “lib”), .libPaths())) library(SparkR)</p>
</div>
<div id="this-script-for-running-spark-code-from-the-r-console-or-rstudio" class="section level1">
<h1>This script for running Spark code from the R Console, or Rstudio</h1>
<p>Sys.getenv() # ensure the environmental variables are set as stated above</p>
<p>library(SparkR, lib.loc = c(file.path(Sys.getenv(“SPARK_HOME”), “R”, “lib”)))</p>
</div>
<div id="note-did-not-run-start-up-file-to-get-this-to-workstart_up-changes-the-java-directory-dont-forget" class="section level1">
<h1>note: did not run Start up file to get this to work…start_up changes the Java directory, don’t forget!!!!</h1>
</div>
<div id="java-make-sure-that-you-set-the-java-directory-with-jre---is-working-currently-version-java8-is-working." class="section level1">
<h1>Java: make sure that you set the Java directory with JRE - is working, currently version Java8 is working.</h1>
<p>sparkR.session(enableHiveSupport = FALSE , master = “local[*]”, sparkHome = Sys.getenv(“SPARK_HOME”) , # this was the missing link!! sparkConfig = list(spark.driver.memory = “2g”, spark.sql.warehouse.dir=“C:\Apps\winutils\winutils-master\hadoop-2.7.1”) # winutils path directory )</p>
<p>df &lt;- as.DataFrame(faithful) str(df) colnames(df) histogram(df$waiting)</p>
<p>createOrReplaceTempView(df, “df”)</p>
<p>waiting_70 &lt;- sql(“select * from df where waiting &gt; 70”) str(waiting_70) collect(waiting_70)</p>
<p>collect(summary(df))</p>
<p>corr(df, “waiting”, “eruptions”)</p>
<p>model &lt;- spark.glm(df, eruptions ~ waiting) summary(model)</p>
<hr />
<p>Kuddos for the Spark image goes to:</p>
<pre class="md"><code>&gt;  - Apache software foundation - https://spark.apache.org/images/spark-logo.eps, Apache License 2.0, https://commons.wikimedia.org/w/index.php?curid=57832155</code></pre>
</div>
