---
title: "Spark Installation on Windows"
author: "Alfredo G Marquez"
date: '2018-05-04'
slug: spark-installation-on-windows
tags: 
  - Spark
  - SparkR
  - installation
  - windows
categories:
- Spark
- SparkR
keywords:
- SparkR
- Spark
- Windows
- Installation
autoThumbnailImage: false
thumbnailImagePosition: "top"
thumbnailImage: https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg
coverImage: "/images/Apache Spark logo.jpeg"
metaAlignment: center
coverMeta: out
summary: "Today I will discuss how to install Apache Spark onto a Windows machine."
draft: false
---



<div id="intro" class="section level1">
<h1>Intro</h1>
<p><em>Today I will discuss how to install Apache Spark onto a Windows machine. I have just walked through the process a second time at work due to a laptop swap and it takes me some time to remember all the steps to get the install right, so I thought I would document the process.</em></p>
<hr />
</div>
<div id="step-1-download-and-installation" class="section level1">
<h1>Step #1: Download and Installation</h1>
<p><strong>Install Spark</strong></p>
<p>First you will need to download Spark, which comes with the package for SparkR. Note, as of this posting, the SparkR package was removed from CRAN, so you can only get SparkR from the Apache website. Spark can be downloaded directly from Apache <a href="http://spark.apache.org/downloads.html">here</a>. I downloaded Spark 2.3.0 Pre-built for Apache Hadoop 2.7 and later.</p>
<p>After downloading, save the zipped file to a directory of choice, and then unzip the file. I have an Apps (“C:\Apps”) folder where I put all the software that I download and install. This make it easy to transition to another laptop as all I have to do is go down the list installing software. I changed/shortened the Spark folder name for simplicity and ease of typing.</p>
<p><img src="/images/Apache_folder_2018_05_15.png" /><!-- --> <br></br></p>
<p><strong>Install Java</strong></p>
<p>As of this writing, Java 9 has been having a ton of issues and is not stable. I was able to use the most recent version of Java 8. You can download the most recent version for Windows at this <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">link</a>. You have to accept their license agreement before downloading.</p>
<p>Unlike Spark, I will let the system determine where to install Java. I feel like this works best to reduce future issues. When you install Java, pay attention to the folder location of the installation. You will need to know this path to create the JAVA_HOME environmental variable.</p>
<p><strong>Install WINUTILS</strong></p>
<p>This step tripped me up for a long time when I first started learning about Spark. Not many blog posts would talk about installing this software. I am not really sure what it does, but if you want more information here is the <a href="https://wiki.apache.org/hadoop/WindowsProblems">ApacheWiki</a>. You can download the winutils software from the github repo <a href="https://github.com/steveloughran/winutils">here</a>. Click on the green button (“clone or download”) on the upper right of the page and download the zip file. I would put this in your “C:Apps\winutils” folder so you can reference it easily (you will need it later). Extract all the files to this location using decompression software like <a href="http://www.peazip.org/">PeaZip</a> or WinZip. PeaZip is free and open source.</p>
<p>Once you have unzipped the file it should look like this: <img src="/images/winutils_folder_2018_05_15.png" /><!-- --> <br></br></p>
</div>
<div id="step-2-set-up-system-environmental-variables-to-run-spark-either-in-windows-or-r" class="section level1">
<h1>Step #2: Set up system environmental variables to run Spark (either in windows, or R):</h1>
<blockquote>
<p>NOTE: if you decide to set the environmental variables through R, you need to remember that it does not change them on your machine, nor does it flow through to the next session unless they are establish in your .Renviron script. The code to do it in R is below, however, I will discuss setting up on your machine next.</p>
</blockquote>
<pre><code>  Sys.setenv(Variable Name = &quot;Variable Value&quot;)
  Sys.setenv(SPARK_HOME = &quot;C:\\Apps\\Apache\\Spark\\spark-2.3.0\\bin&quot;)</code></pre>
<div id="spark_home" class="section level4">
<h4>SPARK_HOME:</h4>
<p>The process to set up the environmental variables on your machine will be similar for Spark as it is for the other variables below, where you provide a variable name, variable value and add it to the Path variable. So I will use Spark as a example that you can apply to the other variables.</p>
<ol style="list-style-type: decimal">
<li>Locate the master or top level file path for the Spark version you downloaded to add to your environmental list. For Spark, the path is in my “C:\Apps” folder, or “C:\Apps\Apache\Spark\spark-2.3.0\”, copy the file path using ctrl + c.</li>
<li>To locate your environmental variables On Windows 10, you can type “environmental” into the search bar at the bottom of you screen, then click on “edit system variables,” and click on the icon that says environmental variables at the bottom. On previous versions of Windows, click on the start menu, right click on computer or control panel, select properties and click on the icon that says “environmental variables”&quot; at the bottom.</li>
</ol>
<p>In both cases your system properties should look like this: <img src="/images/System_Properties_2018_05_15.png" /><!-- --> <br></br></p>
<ol start="3" style="list-style-type: decimal">
<li>Next, add a new variable by clicking on the “New” icon in the User variables section. <img src="/images/System_Add_New_Var_2018_05_15.png" /><!-- --> <br></br></li>
</ol>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Fill out the fields as follows:</li>
</ol></li>
</ul>
<pre><code>  Variable Name: SPARK_HOME
  Variable Value: C:\\Apps\\Apache\\Spark\\spark-2.3.0\\ ( or path to your Spark bin folder)</code></pre>
<p><br></br> <img src="/images/New_Var_SPARK_HOME_1_2018_05_15.png" /><!-- --> <br></br></p>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>The next step is to add the new variable to your user Path variable. In the list of variables in the “user variables” section, you are going to see a variable called “Path.” Select this variable and click on the “Edit” icon. On Windows 10, you get a nice looking list of what is included in the path, but in older versions, you get a long string of variables separated by semi-colons. For example, to add the SPARK_HOME environmental variable to an older version of Windows you would add the following to the end of the Path variable:</li>
</ol></li>
</ul>
<pre><code>  ;%SPARK_HOME%</code></pre>
<ul>
<li>For Windows 10, once you have clicked on the “Edit” icon to edit the “Path” variable you will see the window below. Click on the “New” icon and add the name of the variable as we did above but without the semi-colon, %SPARK_HOME%, in one of the availabe spaces then click “OK” at the bottom of the window. Your “Path” should now have the SPARK_HOME environmental variable as below: <img src="/images/Path_Add_New_Var_2018_05_15.png" /><!-- --> <br></br></li>
</ul>
</div>
<div id="java_home" class="section level4">
<h4>JAVA_HOME:</h4>
<p>Go to the folder where you Java is installed. I installed the 64bit version, so the Java folder is in my C:\Programs directory. If you have installed the JDK version of Java, you will see 2 folders in the Java directory. One for JDK, and the other for JRE. Go into the JRE directory and copy the path, and add it to your system’s environmental variables. NOTE: do not use the path to the “bin” folder as this will cause JVM errors when launching SparkR.</p>
<pre><code>  Variable Name: JAVA_HOME 
  Variable Value: C:\Program Files\Java\jre1.8.0_171\</code></pre>
</div>
<div id="winutils_home" class="section level4">
<h4>WINUTILS_HOME:</h4>
<p>Since I have installed winutils in my C:directory, I will copy the path from there. I will reference the path of the master folder.</p>
<pre><code>  Variable Name: WINUTILS_HOME 
  Variable Value: C:\Apps\winutils\winutils-master\</code></pre>
</div>
<div id="sparkr_driver_r-optional-maybe-needed-on-a-hadoop-system" class="section level4">
<h4>SPARKR_DRIVER_R: (Optional, maybe needed on a hadoop system)</h4>
<p>The SPARKR_DRIVER_R environmental variable I tripped over on my work’s hadoop system when I was looking for something else. I noticed their SPARKR_DRIVER_R was set to their R home directory. You can find your R home directory with the command below in your R console. Note, R may give you a path with a tidla, but if you paste it into a folder browser you can get the full path. Exampe, C://Apps//R//R-34~1.3 means C:\Apps\R\R-3.4.3. As you can see, my R is installed in my C:\Apps folder.</p>
<pre><code>  Sys.getenv() - all environmental variables
  Sys.getenv(&quot;R_HOME&quot;) - Value for the R_HOME environmental variables.</code></pre>
<p>You know have the information to create your own SPARKR_DRIVER_R environmental variable:</p>
<pre><code>  Variable Name: SPARKR_DRIVER_R
  Variable Value: C:\\Apps\\R\\R-3.4.3</code></pre>
<p>Once you have all your environmental variables your Path variable they should look like this: <img src="/images/Path_Env_Vars_Complete_2018_05_15.png" /><!-- --> <br></br></p>
<p>Now that you have everything set up you will have to restart your R and RStudio, meaning close out and open back up, so that they pick up the changes made to the environmental variables.</p>
</div>
</div>
<div id="testing-installation" class="section level1">
<h1>Testing Installation</h1>
<p>First run <code>Sys.getenv()</code> to ensure the environmental variables are set as stated above in you R session. If not go back and figure out what you missed.</p>
<div id="set-library-path-and-load-sparkr-library" class="section level4">
<h4>Set library path and load SparkR library</h4>
<p>This is one way to load your SparkR packages.</p>
<pre class="r"><code>.libPaths(c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;), .libPaths()))
library(SparkR)</code></pre>
<pre><code>## 
## Attaching package: &#39;SparkR&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, filter, lag, na.omit, predict, sd, var, window</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.data.frame, colnames, colnames&lt;-, drop, endsWith,
##     intersect, rank, rbind, sample, startsWith, subset, summary,
##     transform, union</code></pre>
<p>This version of code is commonly used in the SparkR documentation.</p>
<pre class="r"><code>library(SparkR, lib.loc = c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;)))</code></pre>
<p>Use the code that best suites your style of coding, there is no right way.</p>
</div>
<div id="create-a-spark-session" class="section level4">
<h4>Create a Spark Session</h4>
<p>Now that you have the SparkR package loaded, you can create a spark session on you local machine. There are 4 options below that are mandatory.</p>
<ul>
<li>The “enableHiveSupport” must be set to FALSE if you are running on local with no HIVE, otherwise Spark erros trying to fine the HIVE warehouse. If you are running this on a Jupyter notebook on a Hadoop system with HIVE, this should be TRUE.</li>
<li>The “master” variable has to be set to local “local[*]” or to “local[K]” to be used by Spark. This [*] option uses the maximum cores available on your machine, while K you set the number of cores for Spark to use. On hadoop this will be “YARN” or some other configuration depending on your system build.<br />
</li>
<li>The “sparkHome” field also took me a long time to figure out. I came across on some blog a while ago, so I can’t give them credit, but that was a big win. Provide the path to your Spark home directory to this field.</li>
<li>The spark.sql.warehouse.dir in the options list was also a big headache. The way I resolved this was trial and error, copy and pasting what other people were doing to see if it would get me past the errors messages.</li>
</ul>
<pre class="r"><code>sparkR.session(enableHiveSupport = FALSE ,
               master = &quot;local[*]&quot;, 
               sparkHome = Sys.getenv(&quot;SPARK_HOME&quot;) , # this was the missing link!!
               sparkConfig = 
                 list(spark.driver.memory = &quot;2g&quot;, 
                      spark.sql.warehouse.dir=&quot;C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1&quot;) # winutils path directory
)</code></pre>
<pre><code>## Spark package found in SPARK_HOME: C:\Apps\Apache\Spark\spark-2.3.0\</code></pre>
<pre><code>## Launching java with spark-submit command C:\Apps\Apache\Spark\spark-2.3.0\/bin/spark-submit2.cmd   --driver-memory &quot;2g&quot; sparkr-shell C:\Users\marqu\AppData\Local\Temp\RtmpQzcLZR\backend_port3fa8696a47c7</code></pre>
<pre><code>## Java ref type org.apache.spark.sql.SparkSession id 1</code></pre>
<p>All the other settings are optional from what I found, such as the spark.driver.memory. Best practices is that you should set this, but it should work without it.</p>
</div>
<div id="look-at-your-spark-session-stats" class="section level4">
<h4>Look at your Spark Session Stats:</h4>
<p>Paste the url provided below to look at the Spark session running on your machine. You will able to see a history of the Spark commands you submit on this site.</p>
<pre class="r"><code>sparkR.uiWebUrl()</code></pre>
<pre><code>## [1] &quot;http://LAPTOP-OK0GO7H8:4041&quot;</code></pre>
<p>PICTURE HERE</p>
</div>
<div id="load-data-into-spark" class="section level4">
<h4>Load data into Spark</h4>
<p>Load the faithful data into Spark. Notice that the data frame type is “SparkDataFrame,” meaning that this data set is distributed and regular R commands will not be able to handle this object type.</p>
<pre class="r"><code>df &lt;- as.DataFrame(faithful)
str(df)</code></pre>
<pre><code>## &#39;SparkDataFrame&#39;: 2 variables:
##  $ eruptions: num 3.6 1.8 3.333 2.283 4.533 2.883
##  $ waiting  : num 79 54 74 62 85 55</code></pre>
<pre class="r"><code>colnames(df)</code></pre>
<pre><code>## [1] &quot;eruptions&quot; &quot;waiting&quot;</code></pre>
<p>Create a volatile table in Spark memory so that you can reference the data frame with Spark SQL.</p>
<pre class="r"><code>createOrReplaceTempView(df, &quot;df&quot;)</code></pre>
<pre class="r"><code>waiting_70 &lt;- sql(&quot;select * from df where waiting &gt; 70&quot;)
str(waiting_70)</code></pre>
<pre><code>## &#39;SparkDataFrame&#39;: 2 variables:
##  $ eruptions: num 3.6 3.333 4.533 4.7 3.6 4.35
##  $ waiting  : num 79 74 85 88 85 85</code></pre>
<pre class="r"><code>head(collect(waiting_70), 10)</code></pre>
<pre><code>##    eruptions waiting
## 1      3.600      79
## 2      3.333      74
## 3      4.533      85
## 4      4.700      88
## 5      3.600      85
## 6      4.350      85
## 7      3.917      84
## 8      4.200      78
## 9      4.700      83
## 10     4.800      84</code></pre>
<pre class="r"><code>collect(summary(df))</code></pre>
<pre><code>##   summary          eruptions            waiting
## 1   count                272                272
## 2    mean 3.4877830882352936   70.8970588235294
## 3  stddev 1.1413712511052083 13.594973789999392
## 4     min                1.6               43.0
## 5     25%               2.15               58.0
## 6     50%                4.0               76.0
## 7     75%               4.45               82.0
## 8     max                5.1               96.0</code></pre>
<pre class="r"><code>corr(df, &quot;waiting&quot;, &quot;eruptions&quot;)</code></pre>
<pre><code>## [1] 0.9008112</code></pre>
<pre class="r"><code>model &lt;- spark.glm(df, eruptions ~ waiting)
summary(model)</code></pre>
<pre><code>## 
## Deviance Residuals: 
## (Note: These are approximate quantiles with relative error &lt;= 0.01)
##      Min        1Q    Median        3Q       Max  
## -1.29917  -0.38945   0.02252   0.33776   1.19329  
## 
## Coefficients:
##               Estimate  Std. Error  t value  Pr(&gt;|t|)
## (Intercept)  -1.874016   0.1601433  -11.702         0
## waiting       0.075628   0.0022185   34.089         0
## 
## (Dispersion parameter for gaussian family taken to be 0.2465251)
## 
##     Null deviance: 353.039  on 271  degrees of freedom
## Residual deviance:  66.562  on 270  degrees of freedom
## AIC: 395
## 
## Number of Fisher Scoring iterations: 1</code></pre>
<table style="width:7%;">
<colgroup>
<col width="6%" />
</colgroup>
<tbody>
<tr class="odd">
<td># References</td>
</tr>
<tr class="even">
<td>Spark : <a href="https://spark.apache.org/docs/latest/index.html" class="uri">https://spark.apache.org/docs/latest/index.html</a> RPubs : <a href="https://rpubs.com/wendyu/sparkr" class="uri">https://rpubs.com/wendyu/sparkr</a></td>
</tr>
</tbody>
</table>
<p>Kuddos for the Spark image goes to:</p>
<blockquote>
<p>Apache software foundation - <a href="https://spark.apache.org/images/spark-logo.eps" class="uri">https://spark.apache.org/images/spark-logo.eps</a>, Apache License 2.0, <a href="https://commons.wikimedia.org/w/index.php?curid=57832155" class="uri">https://commons.wikimedia.org/w/index.php?curid=57832155</a></p>
</blockquote>
</div>
</div>
