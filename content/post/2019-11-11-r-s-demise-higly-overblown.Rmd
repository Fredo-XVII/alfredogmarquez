---
title: "R's Demise Highly Overblown"
author: "Alfredo G Marquez"
date: '2019-11-11'
coverImage: null
coverMeta: out
coverSize: partial
categories: R
draft: no
keywords:
- R
- popular
- popularity
- trends
metaAlignment: center
slug: r-s-demise-higly-overblown
summary: Today I will provide evidence that R's demise are highly overblown.
tags: R Programming Cranlogs trends popularity
thumbnailImage: https://www.r-project.org/logo/Rlogo.svg
thumbnailImagePosition: top
autoThumbnailImage: no
---

<!--more-->


# 

In this post I will provide evidence that contradicts recent reports that the R Programming language is declining in popularity.

-----

In last few months I have come across several blog posts that suggest that Python is reducing the demand for R. Yet, it contradicts what I am seeing in my day to day life.  Many analyst at work from different parts of the business that are not associated with data science or software engineering are adopting the R programming language because it feels more comfortable than Python and they are not interested in building software or building production pipelines.  These souls are highly ignored by the software community, but they are embraced by the R community because at the end of the day we just care about the data and the stories that they tell, and R is great for telling stories.

-----

# Introduction

As I have read all these posts about python impacting the demand for R and implying that R was on the decline, I though about how I could address this question empirically.  I thought to myself, if the rise in Python for Data Science is decreasing the demand for R for Data Science, then naturally, you would see a decrease in R downloads as users moved from R to Python.  Therefore, my experiment is designed as below:

#### **Design of Experiment:**

> **Null Hypothesis (H0):** No change to R downloads, given the rise in Python.
>
> **Alternative Hypothesis (H1):** R downloads decline, given the rise in Python.
>
> **Basic Assumption:** R downloads are conducted by users intending to use the language. In other words, users are not downloading R just because they enjoy downloading stuff; they actually intend to use the download.

### The Data

Ideally, I would collect the number of times R is download from the CRAN mirrors, as well as, collect the number of times Python is downloaded over time.  My thought was to start the analysis from 2015 to the present in order to capture the rise of both languages.  From this point, you can conduct Granger Causality Test to see if Python is truly impacting the demand for R.  Unfortunately, I was not able to find a downloadable count of Python downloads since 2015.  I did find a page that provide the last 30 days of download statistics, but nothing that you can download that was relevant to this exercise.  

R fared a little better but not much.  CRAN has dozens of mirrors around the world that are self hosted.  In other words, unless the mirror owners publish there download history, there is no way to know how many times R is being downloaded.  Luckily, @RStudio has provided a way to obtain the downloads of R from there mirror in the R package **cranlogs**.   

The **cranlogs** package not only gives you downloads for R itself, but also for packages that are downloaded from the @RStudio cran mirror.  Because @RStudio has an easy GUI interface for downloading packages, and is extremely popular among R users, I will assume that a majority of the package downloads are downloaded by users using @RStudio as an IDE.

In contrast, I will not make the same assumption about installing R itself.  Normally, I install R core from the https://www.r-project.org/ website and then I install @RStudio IDE. This is important because I will use the @RStudio R downloads as a proxy for all R downloads worldwide.

There will be 3 sets of data that I will be working with for this analysis.  First is the data from the cranlogs discussed above.  Second, I will use data from the **rversions** package in order to get R version release dates.  The version dates data will be used to label the graphs and to understand some of the behavior in the downloads.  The third data set that will use comes from the **gtrends** package.  I will use this data as a proxy for what the is being observed by the TOIBE Index and the PYPL Index.  Finally, I research some dates that I felt were relevant after analyzing the data.  For example, Hadley Wickham's introduction of the tidyverse at the useR! 2016 conference at the end of June 2016.

# Analysis

To begin my analysis, I will use the packages loaded below.  

```{r libaries, results='hide', message=FALSE}
# Libraries
library(cranlogs)
library(lubridate)
library(gridExtra)
#library(ggrepel)
library(tidyverse)
library(tsibble)
library(gtrendsR)
library(rversions)
library(scales)
library(kableExtra)

# Options
options(scipen = 999)

# Important Dates
hadley_tidyverse <- as.Date('2016-06-26')
tidy_searches_beg <- as.Date('2016-07-31')
tidyvere_release_1_0 <- as.Date('2016-09-15')
```


```{r, echo=FALSE}
# Download R download data and version dates
#R_ver_hist_raw <- rversions::r_versions() 
R_ver_hist_raw <- readRDS(file = "C:\\Users\\marqu\\OneDrive\\Documents\\blog\\alfredogmarquez\\data\\R_ver_hist_raw.rds")
R_ver_hist_raw$vers_d <- as.POSIXct(R_ver_hist_raw$date)

R_ver_hist <- R_ver_hist_raw %>% dplyr::select(-date) %>% 
  dplyr::mutate(greg_d = lubridate::as_date(vers_d),
                vers_i = forcats::as_factor(version),
                nickname = forcats::as_factor(nickname),
                yr_wk_ver = tsibble::yearweek(as.Date(greg_d))
  ) %>% 
  dplyr::select(version,vers_i,nickname,greg_d,yr_wk_ver) %>% 
  dplyr::filter(greg_d >= as.Date('2015-01-01')) 

R_ver_hist_major <- R_ver_hist %>%  
  dplyr::filter(vers_i %in% c("3.2.0","3.3.0","3.4.0","3.5.0","3.6.0"))

#cran_R_downloads_raw <- cranlogs::cran_downloads('R', from = "2010-01-01", to = lubridate::today())
cran_R_downloads_raw <- readRDS(file = "C:\\Users\\marqu\\OneDrive\\Documents\\blog\\alfredogmarquez\\data\\cran_R_downloads_raw.rds")

cran_R_downloads <- cran_R_downloads_raw %>% 
  dplyr::filter(!version %in% c('devel','release','release.exe','latest')) %>% 
  dplyr::mutate(os_factor = forcats::fct_inorder(forcats::as_factor(os)),
         ver_factor =forcats::fct_inorder(forcats::as_factor(version)),
         ver_lvl = sub("*[.^]", "", version) %>% sub("\\..*", "", .) %>% as.integer(),
         ver_lvl_factor =forcats::fct_inorder(forcats::as_factor(ver_lvl)),
         ver_lvl_top = stringr::str_sub(version,1,1) %>% as.integer(),
         ver_lvl_top_factor =forcats::fct_inorder(forcats::as_factor(ver_lvl_top)),
         year = lubridate::year(date),
         month = lubridate::month(date),
         week = lubridate::week(date),
         yr_mo = tsibble::yearmonth(date),
         yr_wk = tsibble::yearweek(date),
         ver_3_5_f = ifelse(date >= subset(R_ver_hist_major, version == '3.5.0')$greg_d,1,0)
  ) %>% 
  dplyr::filter(yr_wk != max(.$yr_wk)) %>% 
  dplyr::left_join(R_ver_hist, by = c("yr_wk" = "yr_wk_ver")) 
```

Here is a quick look at the download data from the **cranlogs** package.  I created some new features, date factors and period dummy variables, in order to drill into the data graphically. I will show examples of the other data down below.
```{r, echo=FALSE, warning=FALSE}
tail(cran_R_downloads,5) %>% knitr::kable() %>% kableExtra::kable_styling()
```


## Yearly Trend
I was first curious about the overall trend of the data. You can see that R downloads decreased from 2015 to 2016, recovering in 2017 but about 1 million downloads short from its 2015 level.  Then in 2018 R downloads grow by 2 million downloads, or 23%, to 10 million and surpasses its 2015 level by 1 million downloads.  Finally, R downloads have grown by almost 50% to 15 million downloads this year.  I have to admit those totals are more than I would have thought.  This growth might explain why the Michigan CRAN mirror has added a second CRAN mirror; the Michigan Mirror being the closet to my location.  In any case, this evidence contradicts the suggestion that Python is having a negative impact on the adoption of R.

```{r, echo=FALSE, warning=FALSE}
# Build Aggregation Views
# What does the trend look at by Year
down_tot_by_yr <- cran_R_downloads %>% 
  dplyr::group_by(year) %>% 
  dplyr::summarise(total = sum(count)) %>% 
  dplyr::mutate(yoy = total/dplyr::lag(total),
                yoy_perc = (total/dplyr::lag(total) - 1)*100,
                ) %>% 
  tsibble::as_tsibble(index = year) 

g_tot_by_yr <- down_tot_by_yr %>% 
  ggplot(aes(x = year, y = total, color = year)) +
  geom_line() +
  theme_light() + theme(legend.position = "none") +
  xlim(2015, 2020) +
  labs(x = 'Year', y = 'Total Downloads', title = "R Downloads from RStudio Cranlogs by Year") +
  scale_y_continuous(labels = scales::unit_format(unit = "M", scale = 1e-05, digits = 4))


g_tot_by_yoy <- down_tot_by_yr %>% 
  ggplot(aes(x = year, y = yoy_perc, fill = yoy_perc)) +
  geom_col() +
  theme_light() + theme(legend.position = "none") +
  xlim(2015, 2020) +
  annotate(geom = "text", 
           x=down_tot_by_yr$year, 
           y=0, 
           label=paste0(round(down_tot_by_yr$yoy_perc,0),'%'),
           size=5, angle=0, vjust= -.25, hjust= 0.50) +
  labs(x = 'Year', y = 'YOY % Growth') +
  scale_y_continuous(labels = scales::percent_format(scale = 1))

gridExtra::grid.arrange(g_tot_by_yr,g_tot_by_yoy)

```


At this point, I could probably be done because I think that there is sufficient evidence to support that the growth in R is not being negatively impacted by the growth in Python. But there are some other interesting variables in the data that I would like to explore.  


## Natural Experiments: Tale of 2 Bends
```{r, echo=FALSE,warning=FALSE}
# What does the trend look like?
down_tot_by_d <- cran_R_downloads %>% 
  dplyr::group_by(yr_wk,greg_d,forcats::fct_explicit_na(vers_i)) %>% 
  dplyr::summarise(total = sum(count)) %>% 
  tsibble::as_tsibble(index = yr_wk)
```

### 1. Tidyverse's Positive Impact
From the yearly trend above there are 2 interesting behaviors.  First, is the year over year decline in 2016 and the recovery in 2017.  If you look at the graph below you will notice that R downloads are in decline all of 2015 and bottom out in the middle of 2016.  I wondered what happened in 2016 that broke the downward trend.  As I thought about 2016, I remembered that I attended the useR! 2016 Conference in Palo Alto that summer.  At the conference Hadley Wickham officially announced idea of the **Tidyverse**, ending all references to the term **Hadleyverse**, which was the name that had been circling to describe the Tidyverse ecosystem at the time.  We all laughed as we watched Hadleyverse catch fire on screen because it conveyed how humble Hadley truly is.  His speech was given on Wednesday June 29th, 2016 (see below week of June 26th).  The week of July 31st, 2016 is the term "tidyverse" starts to appear in searches on Google Trends (data from gtrend package), or 35 days after Hadley's speech, and Tidyverse 1.0.0 is released on September 15, 2016, or 81 days after Hadley's speech.  In the section below on Google Trends you will see correlation between the tidyverse search term on Google Trends, R downloads bottoming out and rebounding.  

Because economist are not allowed to conduct large scale social experiments with people for moral reasons, we take advantage of natural experiments.  A natural experiment is when there is a policy change or natural event that has a large impact on society.  We can use this event to explain and measure the impact of these large events.  The logic for natural experiments comes from statistics.  In statistics there is this idea that the probability of 2 independents events corresponding together is nearly zero.  In other words, if there is a policy change (treatment) and a corresponding change in an outcome, the probability that these 2 things occur together would be nearly zero if it were to happen by chance.  Although I have no way to prove this, but from the data below I would make the argument that the introduction of the Tidyverse paradigm (treatment) has had a positive impact on R downloads (outcome).  From my experience, dplyr really made R familiar, approachable, easy to pick up.  Base R was much harder to pick up in comparison.  What this shows is that the Tidyverse reduces barriers to entry for new R users by making R easier to learn and use.  In economics this would translate to a reduction in the costs of learning R and thereby increasing the return on investment for each minute spent on learning R.  Furthermore, the cost of using R will continue to decrease as other packages also adopt the concept of the Tidyverse.  One example is the tsibble package, which is becoming one of my new favorites.  By adopting tidy principles, this package has made manipulating time series data much easier than using the zoo or the xts package.
```{r tidyverse_release, echo=FALSE, warning=FALSE}
tidy_fulcrum <- ggplot(down_tot_by_d, aes(x = yr_wk, y = total)) + 
  geom_line() +
  geom_vline(xintercept = hadley_tidyverse, color = 'steelblue', linetype = 4, alpha = 0.75) +
  geom_vline(xintercept = tidy_searches_beg, color = 'red') +
  geom_vline(xintercept = tidyvere_release_1_0, color = 'steelblue', linetype = 4, alpha = 0.75) +
  theme_bw() +
  annotate(geom = "text", 
           x=hadley_tidyverse, 
           y=0, label="Hadley @ useR!2016",
           size=4, angle=90, vjust=-0.8, hjust=-1.35,  color = "steelblue", alpha = 0.75) +
  annotate(geom = "text", 
           x=tidy_searches_beg, 
           y=0, label="Tidyverse Searches Begin",
           size=4, angle=90, vjust=-0.0, hjust=-0.5,  color = "red", alpha = 0.75) +
  annotate(geom = "text", 
           x=tidyvere_release_1_0, 
           y=0, label="Tidyverse 1.0.0",
           size=4, angle=90, vjust=1.5, hjust=-1.90,  color = "steelblue", alpha = 0.75) +
  #theme_light() + theme(legend.position = "none") +
  labs(x = 'Week', y = '', title = "Tidyverse Impacts R Downloads Trend",caption = "")


```

```{r , echo=FALSE, warning=FALSE}
tidy_fulcrum
```

### 2. R 3.5.1 Marks Growth
The second event is the large increase in year over year growth in 2019.  I could not think of any really good natural experiment to explain the large increase in R downloads in the first half of 2018.  So I decided to overlay the release dates of the different versions of R; the blue line represents the minor version releases, and the red dotted line represents patch releases.  There were no major releases during this time.  You can see from the graph below that the release of R 3.5.1 marks the beginning of a change in the slope of R downloads at the beginning of 2018 from a relatively flat trend to an increasing trend.  Now why would this be? 

The R 3.5.0 release was a really big release with many upgrades to the internal components of R, with some changes that made R work faster.  But why no increase in downloads at this point?  Well because the new R upgrades had issues with packages built on the older versions of R.  Thus, a lot of your packages broke or wouldn't even download.  But a lot of these issues were resolved by the R 3.5.1 release.  So this explains why we see a drop in R downloads after the R 3.5.0 release and an increase with the R 3.5.1 release.  But, why would such a release convince potential users, such as data scientists, data analysts and business analysts, to adopt R?  Can you image MS Excel business analysts sitting around waiting for the R 3.5.1 release and jumping up in excitement...I cannot.  This natural experiment is harder to justify than the announcement of the Tidyverse because the Tidyverse changed the way users learned and use R.  What I can say is the release of the R 3.5.1 correspond with something that I cannot currently measure and it marks the beginning of a large adoption in the R programming language. Or maybe I am greatly underestimating the value of speed. 


```{r, echo=FALSE, warning=FALSE}

R351_fulcrum <- down_tot_by_d %>% 
  ggplot(aes(x = yr_wk, y = total)) +
  geom_line() +
  theme_bw() +
  labs(x = 'Week', y = 'Weekly Downloads', title = "R Downloads from RStudio Cranlogs by Week",
       caption = "Blue Line: R minor version releases\n
       Red Line: R version patches \n
       Note: No major versions released during this time.
       ") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  geom_vline(data = R_ver_hist, aes(xintercept = greg_d), linetype = 4, color = "red", alpha = 0.5) +
  geom_vline(data = R_ver_hist_major, aes(xintercept = greg_d), linetype = 1, color = "blue") +
  annotate(geom = "text", 
           x=subset(R_ver_hist_major, version == '3.5.0')$yr_wk, 
           y=0, label=subset(R_ver_hist_major, version == '3.5.0')$version,
           size=4, angle=90, vjust=-0.10, hjust=-0.10, color = "blue", alpha = 0.75) +
  annotate(geom = "text", 
           x=subset(R_ver_hist, version == '3.5.1')$yr_wk, 
           y=0, label=subset(R_ver_hist, version == '3.5.1')$version,
           size=4, angle=90, vjust=-0.10, hjust=-0.10, color = "red", alpha = 0.5)
```

```{r,echo=FALSE, warning=FALSE}
R351_fulcrum
```





## Weekly View by Operating System (OS)

```{r, echo=FALSE}
# What does the trend look like by os
down_tot_by_os <- cran_R_downloads %>% 
  dplyr:: group_by(os_factor,yr_wk) %>% 
  dplyr::summarise(total = sum(count)) %>% 
  dplyr::mutate(yoy = total/dplyr::lag(total, n = 52),
                yoy_perc = (total/dplyr::lag(total,n = 52) - 1)*100
  )

g_os_lvl <- down_tot_by_os %>% 
  ggplot(aes(x = yr_wk, y = total, col = os_factor, group = os_factor)) +
  geom_line() +
  facet_grid(os_factor~., scales = 'free') +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  geom_vline(data = R_ver_hist, aes(xintercept = yr_wk_ver), linetype = 4, color = "red", alpha = 0.5) +
  geom_vline(data = R_ver_hist_major, aes(xintercept = greg_d), linetype = 1, color = "blue") +
  annotate(geom = "text", 
         x=subset(R_ver_hist_major, version == '3.5.0')$yr_wk, 
         y=0, label=subset(R_ver_hist_major, version == '3.5.0')$version,
         size=4, angle=90, vjust=-0.10, hjust=-0.10,  color = "blue", alpha = 0.75) +
  annotate(geom = "text", 
           x=subset(R_ver_hist, version == '3.5.1')$yr_wk, 
           y=0, label=subset(R_ver_hist, version == '3.5.1')$version,
           size=4, angle=90, vjust=-0.10, hjust=-0.10,  color = "red", alpha = 0.5) +
  theme_light() + theme(legend.position = "none") +
  labs(x = 'Week', y = '', title = "R Downloads from RStudio Cranlogs by Week",
       caption = "
       Current R major version is 3.x.x \n
       Blue Line: R minor version releases\n
       Red Dotted Line: R version patches \n       .
       ")
#g_os_lvl

g_os_yoy <- down_tot_by_os %>% 
  ggplot(aes(x = yr_wk, y = yoy_perc, col = os_factor, group = os_factor)) +
  #geom_line() +
  geom_col() +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  facet_grid(os_factor~., scales = 'free') +
  geom_hline(yintercept = 0) +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  geom_vline(data = R_ver_hist, mapping = aes(xintercept = greg_d), linetype = 4, color = "red", alpha = 0.5) +
  geom_vline(data = R_ver_hist_major, aes(xintercept = greg_d), linetype = 1, color = "blue") +
  annotate(geom = "text", 
           x=subset(R_ver_hist_major, version == '3.5.0')$yr_wk, 
           y=0, label=subset(R_ver_hist_major, version == '3.5.0')$version,
           size=4, angle=90, vjust=-0.10, hjust=-0.10,  color = "blue", alpha = 0.75) +
  annotate(geom = "text", 
           x=subset(R_ver_hist, version == '3.5.1')$yr_wk, 
           y=0, label=subset(R_ver_hist, version == '3.5.1')$version,
           size=4, angle=90, vjust=-0.10, hjust=-0.10,  color = "red", alpha = 0.5) +
  theme_light() + theme(legend.position = "none") +
  labs(x = 'Week', y = '', title = "R Downloads Year Over Year Change from RStudio Cranlogs by Week",
       caption = '
       Current R major version is 3.x.x \n
       Blue Line: R minor version releases\n
       Red Dotted Line: R version patches \n  
       ')
#g_os_yoy

#gridExtra::grid.arrange(g_os_lvl, g_os_yoy)

# Summary Stats
down_os_summary <- down_tot_by_os %>% dplyr::left_join(cran_R_downloads %>% select(yr_wk,ver_3_5_f),
                                    by = "yr_wk") %>% 
  dplyr::group_by(os_factor,ver_3_5_f) %>% summarise_all(mean,na.rm = TRUE)

# Function to grab metrics in writing
get_os_summary <- function(os, post351, metric) {
  down_os_summary %>% filter(os_factor == !!os,ver_3_5_f == !!post351) %>% 
    ungroup() %>% select(!!metric) %>% round()
}
```


### 1. Windows Leads the way, Mac Grows Rapidly
Now that we have established that the R 3.5.1 is an inflection point, lets see what is behind the growth in downloads.  From the graphs below, we observe that the operating system (OS) with the most downloads/week is Windows, with an average of `r get_os_summary(os = 'win', post351 = 0, metric = 'total')` downloads/week before the release of R 3.5.1., and `r get_os_summary(os = 'win', post351 = 1, metric = 'total')` downloads/week post the R 3.5.1 release. I was pleasantly surprised to see that Windows was the leading OS for downloads.  Most software engineers that I know use Macs for their development, so who are all these windows users? My guess, and I have no evidence to support this, is that analyst on the business side that traditionally have used excel for their work are now picking up R for the day to day analysis.  R has a bad rap for having an uphill learning curve, but I believe with the tidyverse, this is no longer the case and anyone who says differently hasn't used R in the last 3 years.  In other words, they are speaking from inexperience with the recent improvements in the R ecosystem. **I will show some evidence to this point below.**  I have easily brought people up to speed at work with just 2 packages, dplyr and ggplot2.  Basically, anything that they can do in excel, now they can do with R, and more.

My second surprise from looking at the OS break out was the large increase in Mac users.  I am not sure why this would be the case except that Macs are now more common at work.  For a long time it was very difficult to request a Mac at work, but over the last 3 years or so, it has been much easier to obtain a Mac if you so desire.  If this is happening across industries, then this may explain some of this growth in addition to the university effect where Mac seem to be quite common.  R downloads for Mac averaged `r get_os_summary(os = 'osx', post351 = 0, metric = 'total')` in the period before the R 3.5.1 release and has averaged `r get_os_summary(os = 'osx', post351 = 1, metric = 'total')`, more than doubling its weekly average downloads.  The upward trend for Mac R downloads doesn't seem to waning so there may be more growth to come over the next year.

Finally, I will not discuss the OS labeled "NA" as it is really small, I just present it for completeness.  I was not able to determine if the OS for "SRC" was building R from source, or whether it was for Linux.  In any case, R downloads for "SRC" have been consistent throughout the entire time frame with some large growth after the R 3.5.2 release.  The average download for "SRC" is `r get_os_summary(os = 'src', post351 = 0, metric = 'total')` in the period before R 3.5.1 release and `r get_os_summary(os = 'src', post351 = 1, metric = 'total')` after.



```{r wkly_trend_os, fig.hieght=10, fig.asp= .67, echo=FALSE, warning=FALSE}
g_os_lvl

```

### 2. Skyrocketing Year Over Year Growth
In this section, I will quickly describe the year over year (YOY) growth in R downloads for each type of operating system.  The average weekly YOY growth rate for Windows for the pre-period is `r paste0(get_os_summary(os = 'win', post351 = 0, metric = 'yoy_perc'),'%')` and `r paste0(get_os_summary(os = 'win', post351 = 1, metric = 'yoy_perc'), '%')` for the post period.  The average weekly YOY growth rate for Mac for the pre-period is `r paste0(get_os_summary(os = 'osx', post351 = 0, metric = 'yoy_perc'),'%')` and `r paste0(get_os_summary(os = 'osx', post351 = 1, metric = 'yoy_perc'), '%')` for the post period.  The average weekly YOY growth rate for "SRC" for the pre-period is `r paste0(get_os_summary(os = 'src', post351 = 0, metric = 'yoy_perc'),'%')` and `r paste0(get_os_summary(os = 'src', post351 = 1, metric = 'yoy_perc'), '%')` for the post period.  Imagine for a second you are a product manager, and your product was experiencing 80% plus YOY growth on average every week for nearly 2 years; how ecstatic would you be?



```{r yoy_growth_os, fig.hieght=10, fig.asp= .67, echo=FALSE, warning=FALSE}
g_os_yoy
```




# The Problem with Using Search Trends

The problem using search trends to make predictions is that it makes large assumptions that your consumer remains static. In other words, it ignores how consumers have learned and adapted to new technology.  For example, a few years ago the R programming language was new to the masses, meaning analysts not in tech, nor statisticians, nor data scientist. At this point most curious analyst would search terms such as "R Programming Language" on Google.  But as time passes, the knowledge base is spread by word of mouth and experience.  Those early innovators will spread the word but they won't waste their coworkers time by saying google "R Programming Language."  Instead they would suggest search for "dplyr" or "ggplot2" or "DataCamp" for a learning platform based on their experience.  From my experience, I cannot remember the last time I searched for the "R Programming Language;" its been years.  But I search for R topics every single day.  My searches normally look like this "r package function" or "r function error".  So when I am teaching R to my coworkers this is the format that I use; never do I ever say search for "R Programming Language." 

For example, the TIOBE Index uses **+"\<language\> programming"** for its search query and then counts the hits generated from dozens of search engines, and finally applies a standardization method (see references below).  So for R the search would be "R programming."  

The PLPY Index is a little better, but not much better.  They use the term **"\<language\> tutorial"** to build their index.  I like this better because it suggest that potential users/analysts are interested in  courses where they can learn R suggesting they are in a learning phase.  It tries to get at the way potential R users would go about learning R; it gets at consumer behavior.  However, this search term is quickly undermined by the current R users.  For example, I always recommend DataCamp for learning R, or even Python.  So now my coworkers no longer search for "tutorials" they are now searching for "DataCamp."  There are many other websites that provide training, so once you have found what you are looking for, you don't ever search for "R tutorials" even though you are continue to learn R.


Whatever methods they use are irrelevant; the problem is that the search terms don't accurately reflect the behavior of the consumer.

###Where Trends Fail at Prediction

In the graph below I will show why such "search trend" based predictions are risky for forecasters.  Both the TOIBE Index ("R programming") and the PLPY Index ("R tutorials") show interest for R peaking 2017 and steadily declining over time.  So If you believe that correlation = causation, then you would conclude that the interest for R has been in decline, and R is on the way out.  This would explain the blog post over that last year or so claiming that R was doomed.  Second, they go even further and extrapolate that the cause is due to the adoption of Python.  As I have discussed above, and you can see below, R downloads actually been on the rise since 2017.  This data shows that search trends are not good predictors if you don't know your consumer/user.
```{r}
ggplot(all_trends %>% dplyr::filter(keyword_f %in% c("R programming", "R tutorials", "R Downloads")),
       aes(x = yr_wk, y = hits, group = keyword_f, col = keyword_f)) + 
  geom_line() +
  facet_grid(keyword_f~., scales = 'free') +
  # geom_vline(xintercept = hadley_tidyverse) +
  # geom_vline(xintercept = tidy_searches_beg, color = 'red') +
  # geom_vline(xintercept = tidyvere_release_1_0, color = 'orange') +
  geom_smooth(method = "loess")
```


###Where Trends Succeed at Prediction

Let's discuss a situation where you might know your R consumer/user.  As an R user I know that the main packages for data munging in R are tidyverse and data.table for data in the hundreds of millions of rows. I will focus on tidyverse since I have discussed in this post above.  As an R user I know that I use dplyr and ggplot2 as my core packages for data manipulation and visualization.  So I do a lot of daily searching on "r dplyr function," or "r ggplot2 visualization" for example. So I downloaded their search trends along with tidyverse.


```{r}
ggplot(all_trends %>% dplyr::filter(keyword_f %in% c("dplyr", "ggplot2", "tidyverse", "R Downloads")),
       aes(x = yr_wk, y = hits, group = keyword_f, col = keyword_f)) + 
  geom_line() +
  facet_grid(keyword_f~., scales = 'free') +
  geom_vline(xintercept = hadley_tidyverse) +
  geom_vline(xintercept = tidy_searches_beg, color = 'red') +
  geom_vline(xintercept = tidyvere_release_1_0, color = 'orange') +
  geom_smooth(method = "loess")
```


# Summary 1
- Growth is increasing
- Windows has leading in downloads by a large margin, both Windows and Macs are driving the growth after the R 3.5.1 release.
- Tidyverse changed the course of history.
- Search Trends Lack Rigor

# Reference

  - Cranlogs: 
    - https://cran.r-project.org/web/packages/cranlogs/cranlogs.pdf
    - https://github.com/r-hub/cranlogs.app
  - Hadley Announces Tidyverse at useR! 2016 conference week of 2016-06-26: https://user2016.r-project.org//
  - Tidyverse released 1.0.0 on 2016-09-15: https://blog.rstudio.com/2016/09/15/tidyverse-1-0-0/
  - Google Trend Results: https://trends.google.com/trends/explore?date=2010-01-01%202019-11-23&geo=US&q=%22R%20programming%22,Tidyverse,dplyr,ggplot2
  - TIOBE Index Definition: https://www.tiobe.com/tiobe-index/programming-languages-definition/
  - PYPL Index Definition: http://pypl.github.io/PYPL.html?country=US


# Session Info
```{r}
sessionInfo()
```


