---
title: "Spark Installation on Windows"
author: "Alfredo G Marquez"
date: '2018-05-04'
slug: spark-installation-on-windows
tags: 
  - Spark
  - SparkR
  - installation
  - windows
categories:
- Spark
- SparkR
keywords:
- SparkR
- Spark
- Windows
- Installation
autoThumbnailImage: false
thumbnailImagePosition: "top"
thumbnailImage: https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg
coverImage: https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg
metaAlignment: center
coverMeta: out
summary: "Today I will discuss how to install Apache Spark onto a Windows machine."
draft: false
---


# Intro

> Today I will discuss how to install Apache Spark onto a Windows machine. I have just walked through the process a second time at work due to a laptop swap and it takes me sometime to remember all the steps to get the install right, so I thought I would document the process.

```md
<!--more-->
```
----------

# Step #1: Download and Installation

**Install Spark**

First you will need to download Spark, which comes with the package for SparkR.  Note, as of this posting, the SparkR package was removed from CRAN, so you can only get SparkR from the Apache website. Spark can be downloaded directly from Apache [here](http://spark.apache.org/downloads.html).

After downloading, save the zipped file to a directory of choice, and then unzip the file. I have an Apps ("C:\\Apps") folder where I put all the software that I download and install.  This make it easy to transition to another laptop as all I have to do is go down the list installing software. I changed/shortened the Spark folder name for simplicity and ease of typing.

```{r, echo=FALSE, }
knitr::include_graphics("/images/Apache_folder_2018_05_15.png")
```
<br></br>

**Install Java**

As of this writing, Java 9 has been having a ton of issues and is not stable.  I was able to use the most recent version of Java 8.  You can download the most recent version for Windows at this [link](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html). You have to accept their license agreement before downloadning.  

Unlike Spark, I will let the system determine where to install Java.  I feel like this works best to reduce future issues.

**Install WINUTILS**

This step tripped me up for a long time when I first started to learning about Spark.  Not many blog posts would talk about installing this software.  I am not really sure what it does, but if you want more information here is the [ApacheWiki](https://wiki.apache.org/hadoop/WindowsProblems). You can download the winutils software from the github repo [here](https://github.com/steveloughran/winutils).  Click on the green button ("clone or download") on the upper right of the page and download the zip file.  I would put this in your "C:Apps\\winutils" folder so you can reference it easily (you will need it later).  Extract all the files to this location using decompression software like [PeaZip](http://www.peazip.org/) or WinZip. Pea7 is free and open source.

Once you have unzipped the file it should look like this:
```{r, echo=FALSE, }
knitr::include_graphics("/images/winutils_folder_2018_05_15.png")
```
<br></br>


# Step #2: Set up system environmental variables to run Spark (either in windows, or R):
  > NOTE: if you decide to set the environmental variables through R, you need to remember that it does not change them on your machine, nor does it flow through to the next session unless they are establish in your .Renviron script.  The code to do it in R is below, however, I will discuss setting up on your machine next.
  
  > Sys.setenv(SPARK_HOME = "C:\\Apps\\Apache\\Spark\\spark-2.3.0\\bin")
  
  
#### SPARK_HOME:
The process to set up the environmental variables on your machine will be the same for Spark as it is for the other variables below.  So I will use Spark as a example that you can apply to the other variables.

  1. Locate the "bin" file path for the software you want to add to your environmental list.  For Spark, the path is in my "C:\\Apps" folder, or "C:\\Apps\\Apache\\Spark\\spark-2.3.0\\bin", copy the file path using ctrl + c.
  2. Locate you environmental variables On Windows 10, you can type environmental into the search bar at the bottom of you screen, then click on "edit system variables." On previous versions of Windows, click on the start menu, right click on computer or control panel, select properties.  Click on the icon that says environmental variables at the bottom.
 
Your system properties should look like this:
```{r, echo=FALSE, }
knitr::include_graphics("/images/System_Properties_2018_05_15.png")
```
<br></br>
  3. To add a new variable click on the "Add" icon in the "user variables" section.
    1. Fill out the fields as follows:
    
      - Variable Name: SPARK_HOME
      - Variable Value: C:\Apps\Apache\Spark\spark-2.3.0\bin ( or path to your Spark bin folder)
      
```{r, echo=FALSE, }
knitr::include_graphics("/images/New_Var_SPARK_HOME_1_2018_05_15.png")
```     
<br></br>
    2. The next step is to add the new variable to your user Path variable.  In the list of variables in the "user variables" section, you are going to see a variable called "Path."  Select this variable and click on the "Edit" icon.  On Windows 10, you get a nice looking list of what is included in the path, but in older versions, you get a long string of variables separated by semi-colons. For example, to add the SPARK_HOME environmental variable you would add the following to the end of the Path variable:
    - ;%SPARK_HOME%

  For Windows 10, once you have clicked on the "Edit" icon you see the window below. Click on the "New" icon and add the name of the variable as we did above, %SPARK_HOME%, in one of the lines, then click "OK" at the bottom of the window.
  Your "Path" should now have the SPARK_HOME environmental variable as below:
```{r, echo=FALSE, }
knitr::include_graphics("/images/Path_Add_New_Var_2018_05_15.png")
```     
<br></br>  
  

  ## SPARKR_DRIVER_R: R_HOME from sys.getenv

  ## JAVA_HOME:

  ## WINUTILS_HOME:


# Testing Install

# References

  - RPubs : https://rpubs.com/wendyu/sparkr



# Set library path and load SparkR library
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)


# This script for running Spark code from the R Console, or Rstudio
Sys.getenv() # ensure the environmental variables are set as stated above

library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))

# note: did not run Start up file to get this to work...start_up changes the Java directory, don't forget!!!!
# Java: make sure that you set the Java directory with JRE - is working, currently version Java8 is working.
sparkR.session(enableHiveSupport = FALSE ,
               master = "local[*]", 
               sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
               sparkConfig = list(spark.driver.memory = "2g", 
                                  spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
)

df <- as.DataFrame(faithful)
str(df)
colnames(df)
histogram(df$waiting)

createOrReplaceTempView(df, "df")

waiting_70 <- sql("select * from df where waiting > 70")
str(waiting_70)
collect(waiting_70)

collect(summary(df))

corr(df, "waiting", "eruptions")

model <- spark.glm(df, eruptions ~ waiting)
summary(model)


-----

Kuddos for the Spark image goes to: 
```md
>  - Apache software foundation - https://spark.apache.org/images/spark-logo.eps, Apache License 2.0, https://commons.wikimedia.org/w/index.php?curid=57832155
```