---
title: "Spark Installation on Windows"
author: "Alfredo G Marquez"
date: '2018-05-04'
slug: spark-installation-on-windows
tags: 
  - Spark
  - SparkR
  - installation
  - windows
categories:
- Spark
- SparkR
keywords:
- SparkR
- Spark
- Windows
- Installation
autoThumbnailImage: false
thumbnailImagePosition: "top"
thumbnailImage: https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg
coverImage: https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg
metaAlignment: center
coverMeta: out
summary: "Today I will discuss how to install Apache Spark onto a Windows machine."
draft: false
---


# Intro

> Today I will discuss how to install Apache Spark onto a Windows machine. I have just walked through the process a second time at work due to a laptop swap and it takes me sometime to remember all the steps to get the install right, so I thought I would document the process.

```md
<!--more-->
```
----------

# Step #1: Download and Installation

**Install Spark**

First you will need to download Spark, which comes with the package for SparkR.  Note, as of this posting, the SparkR package was removed from CRAN, so you can only get SparkR from the Apache website. Spark can be downloaded directly from Apache [here](http://spark.apache.org/downloads.html).

After downloading, save the zipped file to a directory of choice, and then unzip the file. I have an Apps ("C:\\Apps") folder where I put all the software that I download and install.  This make it easy to transition to another laptop as all I have to do is go down the list installing software. I changed/shortened the Spark folder name for simplicity and ease of typing.

```{r, echo=FALSE, }
knitr::include_graphics("/images/Apache_folder_2018_05_15.png")
```
<br></br>

**Install Java**

As of this writing, Java 9 has been having a ton of issues and is not stable.  I was able to use the most recent version of Java 8.  You can download the most recent version for Windows at this [link](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html). You have to accept their license agreement before downloadning.  

Unlike Spark, I will let the system determine where to install Java.  I feel like this works best to reduce future issues.  When you install Java, pay attention to the folder location of the installation.  This is where you will find the bin folder. You will need to know this path to create the JAVA_HOME environmental variable.

**Install WINUTILS**

This step tripped me up for a long time when I first started to learning about Spark.  Not many blog posts would talk about installing this software.  I am not really sure what it does, but if you want more information here is the [ApacheWiki](https://wiki.apache.org/hadoop/WindowsProblems). You can download the winutils software from the github repo [here](https://github.com/steveloughran/winutils).  Click on the green button ("clone or download") on the upper right of the page and download the zip file.  I would put this in your "C:Apps\\winutils" folder so you can reference it easily (you will need it later).  Extract all the files to this location using decompression software like [PeaZip](http://www.peazip.org/) or WinZip. Pea7 is free and open source.

Once you have unzipped the file it should look like this:
```{r, echo=FALSE, }
knitr::include_graphics("/images/winutils_folder_2018_05_15.png")
```
<br></br>


# Step #2: Set up system environmental variables to run Spark (either in windows, or R):
  > NOTE: if you decide to set the environmental variables through R, you need to remember that it does not change them on your machine, nor does it flow through to the next session unless they are establish in your .Renviron script.  The code to do it in R is below, however, I will discuss setting up on your machine next.
 
  
```
  Sys.setenv(Variable Name = "Variable Value")
  Sys.setenv(SPARK_HOME = "C:\\Apps\\Apache\\Spark\\spark-2.3.0\\bin")
```
  
#### SPARK_HOME:
The process to set up the environmental variables on your machine will be the same for Spark as it is for the other variables below.  So I will use Spark as a example that you can apply to the other variables.

  1. Locate the "bin" file path for the software you want to add to your environmental list.  For Spark, the path is in my "C:\\Apps" folder, or "C:\\Apps\\Apache\\Spark\\spark-2.3.0\\bin", copy the file path using ctrl + c.
  2. Locate you environmental variables On Windows 10, you can type environmental into the search bar at the bottom of you screen, then click on "edit system variables." On previous versions of Windows, click on the start menu, right click on computer or control panel, select properties.  Click on the icon that says environmental variables at the bottom.
 
Your system properties should look like this:
```{r, echo=FALSE, }
knitr::include_graphics("/images/System_Properties_2018_05_15.png")
```
<br></br>


  3. Next, add a new variable by clicking on the "Add" icon in the "user variables" section(not shown).
* a. Fill out the fields as follows:
    
```
  Variable Name: SPARK_HOME
  Variable Value: C:\\Apps\\Apache\\Spark\\spark-2.3.0\\bin ( or path to your Spark bin folder)
```

<br></br>
```{r, echo=FALSE, }
knitr::include_graphics("/images/New_Var_SPARK_HOME_1_2018_05_15.png")
```     
<br></br>

* b. The next step is to add the new variable to your user Path variable.  In the list of variables in the "user variables" section, you are going to see a variable called "Path."  Select this variable and click on the "Edit" icon.  On Windows 10, you get a nice looking list of what is included in the path, but in older versions, you get a long string of variables separated by semi-colons. For example, to add the SPARK_HOME environmental variable you would add the following to the end of the Path variable:
  
```  
  ;%SPARK_HOME%
```

  For Windows 10, once you have clicked on the "Edit" icon to edit the "Path" variable you will see the window below. Click on the "New" icon and add the name of the variable as we did above but without the semi-colon, %SPARK_HOME%, in one of the lines then click "OK" at the bottom of the window.
  Your "Path" should now have the SPARK_HOME environmental variable as below:
```{r, echo=FALSE, }
knitr::include_graphics("/images/Path_Add_New_Var_2018_05_15.png")
```     
<br></br>  
  

#### JAVA_HOME:
Go to the folder where you Java is installed. I installed the 64bit version, so the Java folder is in my C:\\Programs directory.  If you have installed the JDK version of Java, you will see 2 folders in the Java directory. One for JDK, and the other for JRE. Go into the JRE directory and locate the path to the bin folder.  Copy and add it to your system's environmental variables. (See the picture below)

```
  Variable Name: JAVA_HOME 
  Variable Value: 
```

#### WINUTILS_HOME:
Since I have installed winutils in my C:\Apps directory, I will copy the path from there.
Winutils is different than the programs above as it has no "bin" folder. Because I have not installation of hadoop to deal with, I will call the most recent version in my R code.  More on that later. In this case, I will reference the path of the master folder. 

```
  Variable Name: WINUTILS_HOME 
  Variable Value: C:\\Apps\\winutils\\winutils-master
```

#### SPARKR_DRIVER_R: (Optional)
The SPARKR_DRIVER_R environmental variable I tripped over on my work's hadoop system when I was looking for something else.  I noticed their SPARKR_DRIVER_R was set to their R home directory.  You can find your R home directory with the command below in your R console. Note, R may give you a path with a tidla, but if you paste it into a folder browser you can get the full path.  Exampe, C://Apps//R//R-34~1.3 means C:\\Apps\\R\\R-3.4.3. As you can see, my R is installed in my C:\\Apps folder.

```
  Sys.getenv() - all environmental variables
  Sys.getenv("R_HOME") - Value for the R_HOME environmental variables.
```  
  
You know have the information to create your own SPARKR_DRIVER_R environmental variable:

```
  Variable Name: SPARKR_DRIVER_R
  Variable Value: C:\\Apps\\R\\R-3.4.3
```

Once you have all your environmental variables your Path variable they should look like this:
```{r, echo=FALSE, }
knitr::include_graphics("/images/Path_Env_Vars_Complete_2018_05_15.png")
```     
<br></br>  

# Testing Install

# References

  - RPubs : https://rpubs.com/wendyu/sparkr



# Set library path and load SparkR library
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)


# This script for running Spark code from the R Console, or Rstudio
Sys.getenv() # ensure the environmental variables are set as stated above

library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))

# note: did not run Start up file to get this to work...start_up changes the Java directory, don't forget!!!!
# Java: make sure that you set the Java directory with JRE - is working, currently version Java8 is working.
sparkR.session(enableHiveSupport = FALSE ,
               master = "local[*]", 
               sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
               sparkConfig = list(spark.driver.memory = "2g", 
                                  spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
)

df <- as.DataFrame(faithful)
str(df)
colnames(df)
histogram(df$waiting)

createOrReplaceTempView(df, "df")

waiting_70 <- sql("select * from df where waiting > 70")
str(waiting_70)
collect(waiting_70)

collect(summary(df))

corr(df, "waiting", "eruptions")

model <- spark.glm(df, eruptions ~ waiting)
summary(model)


-----

Kuddos for the Spark image goes to: 
```md
>  - Apache software foundation - https://spark.apache.org/images/spark-logo.eps, Apache License 2.0, https://commons.wikimedia.org/w/index.php?curid=57832155
```