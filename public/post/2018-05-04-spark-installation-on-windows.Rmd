---
title: "Spark Installation on Windows"
author: "Alfredo G Marquez"
date: '2018-05-04'
slug: spark-installation-on-windows
tags: 
  - Spark
  - SparkR
  - installation
  - windows
categories:
- Spark
- SparkR
keywords:
- SparkR
- Spark
- Windows
- Installation
autoThumbnailImage: false
thumbnailImagePosition: "top"
thumbnailImage: https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg
coverImage: https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg
metaAlignment: center
coverMeta: out
summary: "Today I will discuss how to install Apache Spark onto a Windows machine."
draft: false
---


# Intro

> Today I will discuss how to install Apache Spark onto a Windows machine. I have just walked through the process a second time at work due to a laptop swap and it takes me sometime to remember all the steps to get the install right, so I thought I would document the process.

```md
<!--more-->
```
----------

# Download and Installation

**Install Spark**
First you will need to download Spark, which comes with the package for SparkR.  Note, as of this posting, the SparkR package was removed from CRAN, so you can only get SparkR from the Apache website. Spark can be downloaded directly from Apache [here](http://spark.apache.org/downloads.html).

After downloading, save the zipped file to a directory of choice, and then unzip the file. I have an Apps (C:\Apps) folder where I put all the software that I download and install.  This make it easy to transition to another laptop as all I have to do is go down the list installing software.  

```{r, echo=FALSE, }
knitr::include_graphics("\\img\\Apache_folder.png")
```
<br></br>

**Install Java**
As of this writing, Java 9 has been having a ton of issues and is not stable.  I was able to use the most recent version of Java 8.  You can download the most recent version for Windows at this [link](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html). You have to accept their license agreement before downloadning.  

Unlike Spark, I will let the system determine where to install Java.  I feel like this works best to reduce future issues.

# We can then set up Spark in R environment.

# Set system environmental variables to run Spark (either in windows, or R):
  ## SPARK_HOME:
  Sys.setenv(SPARK_HOME = "C:\\Apps\\Apache\\Spark\\spark-2.3.0\\bin")

  ## SPARKR_DRIVER_R: R_HOME from sys.getenv

  ## JAVA_HOME:

  ## WINUTILS_HOME:


# Testing Install

# References

  - RPubs : https://rpubs.com/wendyu/sparkr



# Set library path and load SparkR library
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)


# This script for running Spark code from the R Console, or Rstudio
Sys.getenv() # ensure the environmental variables are set as stated above

library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))

# note: did not run Start up file to get this to work...start_up changes the Java directory, don't forget!!!!
# Java: make sure that you set the Java directory with JRE - is working, currently version Java8 is working.
sparkR.session(enableHiveSupport = FALSE ,
               master = "local[*]", 
               sparkHome = Sys.getenv("SPARK_HOME") , # this was the missing link!!
               sparkConfig = list(spark.driver.memory = "2g", 
                                  spark.sql.warehouse.dir="C:\\Apps\\winutils\\winutils-master\\hadoop-2.7.1") # winutils path directory
)

df <- as.DataFrame(faithful)
str(df)
colnames(df)
histogram(df$waiting)

createOrReplaceTempView(df, "df")

waiting_70 <- sql("select * from df where waiting > 70")
str(waiting_70)
collect(waiting_70)

collect(summary(df))

corr(df, "waiting", "eruptions")

model <- spark.glm(df, eruptions ~ waiting)
summary(model)


-----

Kuddos for the Spark image goes to: 
```md
>  - Apache software foundation - https://spark.apache.org/images/spark-logo.eps, Apache License 2.0, https://commons.wikimedia.org/w/index.php?curid=57832155
```